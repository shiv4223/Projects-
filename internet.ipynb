{"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing important libararies\nimport string\nimport re\nfrom pickle import dump\nfrom unicodedata import normalize\nfrom numpy import array\n \n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n    file = open(filename, mode='rt', encoding='utf-8')\n\t# read all text\n    text = file.read()\n\t# close the file\n    file.close()\n    return text\n \n# split a loaded document into sentences\ndef to_pairs(doc):\n    lines = doc.strip().split('\\n')\n    pairs = [line.split('\\t') for line in  lines]\n    return pairs\n \n# clean a list of lines\ndef clean_pairs(lines):\n    cleaned = list()\n\t# prepare regex for char filtering\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n\t# prepare translation table for removing punctuation\n    table = str.maketrans('', '', string.punctuation)\n    for pair in lines:\n        clean_pair = list()\n        for line in pair:\n\t\t\t# normalize unicode characters\n            line = normalize('NFD', line).encode('ascii', 'ignore')\n            line = line.decode('UTF-8')\n            # tokenize on white space\n            line = line.split()\n\t\t\t# convert to lowercase\n            line = [word.lower() for word in line]\n\t\t\t# remove punctuation from each token\n            line = [word.translate(table) for word in line]\n\t\t\t# remove non-printable chars form each token\n            line = [re_print.sub('', w) for w in line]\n\t\t\t# remove tokens with numbers in them\n            line = [word for word in line if word.isalpha()]\n\t\t\t# store as string\n            clean_pair.append(' '.join(line))\n        cleaned.append(clean_pair)\n    return array(cleaned)\n \n# save a list of clean sentences to file\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)\n \n# load dataset\nfilename = '/kaggle/input/french/fra.txt'\ndoc = load_doc(filename)\n# split into english-german pairs\npairs = to_pairs(doc)\n# clean sentences\nclean_pairs = clean_pairs(pairs)\n# save clean pairs to file\nsave_clean_data(clean_pairs, 'english-french.pkl')\n# spot check\nfor i in range(100):\n    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))","metadata":{"execution":{"iopub.status.busy":"2022-11-05T18:17:07.902800Z","iopub.execute_input":"2022-11-05T18:17:07.903264Z","iopub.status.idle":"2022-11-05T18:17:19.468386Z","shell.execute_reply.started":"2022-11-05T18:17:07.903221Z","shell.execute_reply":"2022-11-05T18:17:19.467046Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Saved: english-french.pkl\n[go] => [va]\n[go] => [marche]\n[go] => [en route]\n[go] => [bouge]\n[hi] => [salut]\n[hi] => [salut]\n[run] => [cours]\n[run] => [courez]\n[run] => [prenez vos jambes a vos cous]\n[run] => [file]\n[run] => [filez]\n[run] => [cours]\n[run] => [fuyez]\n[run] => [fuyons]\n[run] => [cours]\n[run] => [courez]\n[run] => [prenez vos jambes a vos cous]\n[run] => [file]\n[run] => [filez]\n[run] => [cours]\n[run] => [fuyez]\n[run] => [fuyons]\n[who] => [qui]\n[wow] => [ca alors]\n[wow] => [waouh]\n[wow] => [wah]\n[duck] => [a terre]\n[duck] => [baissetoi]\n[duck] => [baissezvous]\n[fire] => [au feu]\n[help] => [a laide]\n[hide] => [cachetoi]\n[hide] => [cachezvous]\n[jump] => [saute]\n[jump] => [saute]\n[stop] => [ca suffit]\n[stop] => [stop]\n[stop] => [arretetoi]\n[wait] => [attends]\n[wait] => [attendez]\n[wait] => [attendez]\n[wait] => [attends]\n[wait] => [attendez]\n[wait] => [attends]\n[wait] => [attendez]\n[begin] => [commencez]\n[begin] => [commence]\n[go on] => [poursuis]\n[go on] => [continuez]\n[go on] => [poursuivez]\n[hello] => [bonjour]\n[hello] => [salut]\n[i see] => [je comprends]\n[i see] => [aha]\n[i try] => [jessaye]\n[i won] => [jai gagne]\n[i won] => [je lai emporte]\n[i won] => [jai gagne]\n[oh no] => [oh non]\n[relax] => [calmetoi]\n[relax] => [detendstoi]\n[relax] => [detendezvous]\n[relax] => [relaxe max]\n[relax] => [cool raoul]\n[relax] => [du calme]\n[relax] => [relaxe]\n[relax] => [calmezvous]\n[relax] => [detendstoi]\n[relax] => [detendstoi]\n[relax] => [du calme]\n[relax] => [tranquille]\n[smile] => [souriez]\n[smile] => [souris pour la camera]\n[smile] => [souriez]\n[sorry] => [pardon]\n[attack] => [attaque]\n[attack] => [attaquez]\n[attack] => [a lattaque]\n[attack] => [a lattaque]\n[buy it] => [achetezla]\n[buy it] => [achetele]\n[buy it] => [achetezle]\n[buy it] => [achetela]\n[cheers] => [sante]\n[cheers] => [a votre sante]\n[cheers] => [merci]\n[cheers] => [tchintchin]\n[eat it] => [mangezle]\n[eat it] => [mangele]\n[get up] => [levetoi]\n[get up] => [levetoi]\n[get up] => [debout]\n[go now] => [va maintenant]\n[go now] => [allezy maintenant]\n[go now] => [vasy maintenant]\n[got it] => [jai pige]\n[got it] => [compris]\n[got it] => [aha]\n[got it] => [pige]\n[got it] => [compris]\n","output_type":"stream"}]},{"cell_type":"code","source":"from pickle import dump,load\nfrom numpy.random import rand\nfrom numpy.random import shuffle\n \n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n \n# save a list of clean sentences to file\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)\n # load dataset\nraw_dataset = load_clean_sentences('english-french.pkl')\n \n# reduce dataset size\nn_sentences = 10000\ndataset = raw_dataset[:n_sentences, :]\n# random shuffle\nshuffle(dataset)\n# split into train/test\ntrain, test = dataset[:9000], dataset[9000:]\n# save\nsave_clean_data(dataset, 'english-french-both.pkl')\nsave_clean_data(train, 'english-french-train.pkl')\nsave_clean_data(test, 'english-french-test.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-05T18:17:19.471074Z","iopub.execute_input":"2022-11-05T18:17:19.472128Z","iopub.status.idle":"2022-11-05T18:17:22.853683Z","shell.execute_reply.started":"2022-11-05T18:17:19.472079Z","shell.execute_reply":"2022-11-05T18:17:22.852321Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Saved: english-french-both.pkl\nSaved: english-french-train.pkl\nSaved: english-french-test.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"from numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.callbacks import ModelCheckpoint\n \n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n \n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n \n# max sentence length\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)\n \n# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n\t# integer encode sequences\n    X = tokenizer.texts_to_sequences(lines)\n\t# pad sequences with 0 values\n    X = pad_sequences(X, maxlen=length, padding='post')\n    return X\n \n# one hot encode target sequence\ndef encode_output(sequences, vocab_size):\n    ylist = list()\n    for sequence in sequences:\n        encoded = to_categorical(sequence, num_classes=vocab_size)\n        ylist.append(encoded)\n    y = array(ylist)\n    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    return y\n \n# define NMT model\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n    model = Sequential()\n    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n    model.add(LSTM(n_units))\n    model.add(RepeatVector(tar_timesteps))\n    model.add(LSTM(n_units, return_sequences=True))\n    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n    return model\n \n# load datasets\ndataset = load_clean_sentences('english-french-both.pkl')\ntrain = load_clean_sentences('english-french-train.pkl')\ntest = load_clean_sentences('english-french-test.pkl')\n \n \n# prepare english tokenizer\neng_tokenizer = create_tokenizer(dataset[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\nprint('English Vocabulary Size: %d' % eng_vocab_size)\nprint('English Max Length: %d' % (eng_length))\n# prepare german tokenizer\nger_tokenizer = create_tokenizer(dataset[:, 1])\nger_vocab_size = len(ger_tokenizer.word_index) + 1\nger_length = max_length(dataset[:, 1])\nprint('French Vocabulary Size: %d' % ger_vocab_size)\nprint('French Max Length: %d' % (ger_length))\n \n# prepare training data\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\ntrainY = encode_output(trainY, eng_vocab_size)\n# prepare validation data\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\ntestY = encode_output(testY, eng_vocab_size)\n \n# define model\nmodel = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['acc'])\n# summarize defined model\nprint(model.summary())\nplot_model(model, to_file='model.png', show_shapes=True)\n# fit model\nfilename = 'model.h5'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-05T18:17:22.856251Z","iopub.execute_input":"2022-11-05T18:17:22.856762Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"English Vocabulary Size: 2006\nEnglish Max Length: 4\nFrench Vocabulary Size: 4314\nFrench Max Length: 10\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 10, 256)           1104384   \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 256)               525312    \n_________________________________________________________________\nrepeat_vector_2 (RepeatVecto (None, 4, 256)            0         \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 4, 256)            525312    \n_________________________________________________________________\ntime_distributed_2 (TimeDist (None, 4, 2006)           515542    \n=================================================================\nTotal params: 2,670,550\nTrainable params: 2,670,550\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/30\n141/141 - 24s - loss: 4.7306 - acc: 0.3624 - val_loss: 3.9391 - val_acc: 0.3830\n\nEpoch 00001: val_loss improved from inf to 3.93912, saving model to model.h5\nEpoch 2/30\n141/141 - 13s - loss: 3.7888 - acc: 0.3856 - val_loss: 3.7129 - val_acc: 0.3932\n\nEpoch 00002: val_loss improved from 3.93912 to 3.71288, saving model to model.h5\nEpoch 3/30\n141/141 - 14s - loss: 3.5649 - acc: 0.4014 - val_loss: 3.5632 - val_acc: 0.4148\n\nEpoch 00003: val_loss improved from 3.71288 to 3.56323, saving model to model.h5\nEpoch 4/30\n141/141 - 13s - loss: 3.3552 - acc: 0.4230 - val_loss: 3.4096 - val_acc: 0.4355\n\nEpoch 00004: val_loss improved from 3.56323 to 3.40964, saving model to model.h5\nEpoch 5/30\n141/141 - 13s - loss: 3.1648 - acc: 0.4557 - val_loss: 3.2682 - val_acc: 0.4622\n\nEpoch 00005: val_loss improved from 3.40964 to 3.26823, saving model to model.h5\nEpoch 6/30\n141/141 - 15s - loss: 2.9791 - acc: 0.4801 - val_loss: 3.1348 - val_acc: 0.4880\n\nEpoch 00006: val_loss improved from 3.26823 to 3.13475, saving model to model.h5\nEpoch 7/30\n141/141 - 13s - loss: 2.7963 - acc: 0.5052 - val_loss: 3.0056 - val_acc: 0.5067\n\nEpoch 00007: val_loss improved from 3.13475 to 3.00558, saving model to model.h5\nEpoch 8/30\n141/141 - 14s - loss: 2.6181 - acc: 0.5262 - val_loss: 2.8763 - val_acc: 0.5240\n\nEpoch 00008: val_loss improved from 3.00558 to 2.87633, saving model to model.h5\nEpoch 9/30\n141/141 - 13s - loss: 2.4496 - acc: 0.5479 - val_loss: 2.7793 - val_acc: 0.5362\n\nEpoch 00009: val_loss improved from 2.87633 to 2.77925, saving model to model.h5\nEpoch 10/30\n141/141 - 14s - loss: 2.2893 - acc: 0.5650 - val_loss: 2.7017 - val_acc: 0.5440\n\nEpoch 00010: val_loss improved from 2.77925 to 2.70168, saving model to model.h5\nEpoch 11/30\n141/141 - 14s - loss: 2.1443 - acc: 0.5808 - val_loss: 2.6264 - val_acc: 0.5570\n\nEpoch 00011: val_loss improved from 2.70168 to 2.62637, saving model to model.h5\nEpoch 12/30\n141/141 - 14s - loss: 2.0127 - acc: 0.5990 - val_loss: 2.5783 - val_acc: 0.5660\n\nEpoch 00012: val_loss improved from 2.62637 to 2.57834, saving model to model.h5\nEpoch 13/30\n141/141 - 14s - loss: 1.8965 - acc: 0.6137 - val_loss: 2.5317 - val_acc: 0.5710\n\nEpoch 00013: val_loss improved from 2.57834 to 2.53166, saving model to model.h5\nEpoch 14/30\n141/141 - 15s - loss: 1.7759 - acc: 0.6304 - val_loss: 2.4845 - val_acc: 0.5763\n\nEpoch 00014: val_loss improved from 2.53166 to 2.48452, saving model to model.h5\nEpoch 15/30\n141/141 - 15s - loss: 1.6629 - acc: 0.6462 - val_loss: 2.4211 - val_acc: 0.5863\n\nEpoch 00015: val_loss improved from 2.48452 to 2.42112, saving model to model.h5\nEpoch 16/30\n141/141 - 15s - loss: 1.5537 - acc: 0.6646 - val_loss: 2.3836 - val_acc: 0.5922\n\nEpoch 00016: val_loss improved from 2.42112 to 2.38360, saving model to model.h5\nEpoch 17/30\n141/141 - 15s - loss: 1.4504 - acc: 0.6793 - val_loss: 2.3345 - val_acc: 0.6012\n\nEpoch 00017: val_loss improved from 2.38360 to 2.33448, saving model to model.h5\nEpoch 18/30\n141/141 - 14s - loss: 1.3479 - acc: 0.6984 - val_loss: 2.3101 - val_acc: 0.6008\n\nEpoch 00018: val_loss improved from 2.33448 to 2.31009, saving model to model.h5\nEpoch 19/30\n141/141 - 15s - loss: 1.2534 - acc: 0.7179 - val_loss: 2.2773 - val_acc: 0.6125\n\nEpoch 00019: val_loss improved from 2.31009 to 2.27730, saving model to model.h5\nEpoch 20/30\n141/141 - 14s - loss: 1.1608 - acc: 0.7361 - val_loss: 2.2463 - val_acc: 0.6108\n\nEpoch 00020: val_loss improved from 2.27730 to 2.24629, saving model to model.h5\nEpoch 21/30\n141/141 - 15s - loss: 1.0753 - acc: 0.7541 - val_loss: 2.2270 - val_acc: 0.6208\n\nEpoch 00021: val_loss improved from 2.24629 to 2.22700, saving model to model.h5\nEpoch 22/30\n141/141 - 14s - loss: 0.9960 - acc: 0.7692 - val_loss: 2.2219 - val_acc: 0.6235\n\nEpoch 00022: val_loss improved from 2.22700 to 2.22189, saving model to model.h5\nEpoch 23/30\n141/141 - 15s - loss: 0.9215 - acc: 0.7847 - val_loss: 2.1772 - val_acc: 0.6308\n\nEpoch 00023: val_loss improved from 2.22189 to 2.17723, saving model to model.h5\nEpoch 24/30\n141/141 - 14s - loss: 0.8456 - acc: 0.8016 - val_loss: 2.1692 - val_acc: 0.6385\n\nEpoch 00024: val_loss improved from 2.17723 to 2.16920, saving model to model.h5\nEpoch 25/30\n141/141 - 14s - loss: 0.7816 - acc: 0.8162 - val_loss: 2.1585 - val_acc: 0.6405\n\nEpoch 00025: val_loss improved from 2.16920 to 2.15847, saving model to model.h5\nEpoch 26/30\n141/141 - 14s - loss: 0.7204 - acc: 0.8308 - val_loss: 2.1535 - val_acc: 0.6405\n\nEpoch 00026: val_loss improved from 2.15847 to 2.15351, saving model to model.h5\nEpoch 27/30\n141/141 - 14s - loss: 0.6618 - acc: 0.8448 - val_loss: 2.1356 - val_acc: 0.6482\n\nEpoch 00027: val_loss improved from 2.15351 to 2.13556, saving model to model.h5\nEpoch 28/30\n141/141 - 14s - loss: 0.6098 - acc: 0.8560 - val_loss: 2.1357 - val_acc: 0.6518\n\nEpoch 00028: val_loss did not improve from 2.13556\nEpoch 29/30\n141/141 - 14s - loss: 0.5629 - acc: 0.8659 - val_loss: 2.1322 - val_acc: 0.6513\n\nEpoch 00029: val_loss improved from 2.13556 to 2.13224, saving model to model.h5\nEpoch 30/30\n","output_type":"stream"}]},{"cell_type":"code","source":"from numpy import array\nfrom numpy import argmax\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n \n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n \n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n \n# max sentence length\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)\n \n# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    X = tokenizer.texts_to_sequences(lines)\n\t# pad sequences with 0 values\n    X = pad_sequences(X, maxlen=length, padding='post')\n    return X\n \n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n \n# generate target given source sequence\ndef predict_sequence(model, tokenizer, source):\n    prediction = model.predict(source, verbose=0)[0]\n    integers = [argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ' '.join(target)\n \n# evaluate the skill of the model\ndef evaluate_model(model, tokenizer, sources, raw_dataset):\n    actual, predicted = list(), list()\n    for i, source in enumerate(sources):\n        # translate encoded source text\n        source = source.reshape((1, source.shape[0]))\n        translation = predict_sequence(model, eng_tokenizer, source)\n        raw_target, raw_src = raw_dataset[i]\n        if i < 10:\n            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n        actual.append([raw_target.split()])\n        predicted.append(translation.split())\n\t# calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n \n# load datasets\ndataset = load_clean_sentences('english-french-both.pkl')\ntrain = load_clean_sentences('english-french-train.pkl')\ntest = load_clean_sentences('english-french-test.pkl')\n# prepare english tokenizer\neng_tokenizer = create_tokenizer(dataset[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\n# prepare german tokenizer\nger_tokenizer = create_tokenizer(dataset[:, 1])\nger_vocab_size = len(ger_tokenizer.word_index) + 1\nger_length = max_length(dataset[:, 1])\n# prepare data\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n \n# load model\nmodel = load_model('model.h5')\n# test on some training sequences\nprint('train')\nevaluate_model(model, eng_tokenizer, trainX, train)\n# test on some test sequences\nprint('test')\nevaluate_model(model, eng_tokenizer, testX, test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}