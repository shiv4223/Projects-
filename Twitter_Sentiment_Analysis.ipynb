{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTWsEAkvlk3z"
   },
   "source": [
    "#**Twitter Sentiment Analysis : US Airline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqNJRp5ol3FM"
   },
   "source": [
    "## **Importing requires libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlSvJmM5lj-e",
    "outputId": "08d5a83a-c418-47d5-cd0d-5c69f78be338"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shivanshsharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shivanshsharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shivanshsharma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shivanshsharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Guq-Nxknzzt"
   },
   "source": [
    "## **Reading the training and testing csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCzP4Mj39CBm",
    "outputId": "2a0e6e9e-8695-4118-9468-c985c9cdcfda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980, 12), (3660, 11))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv(\"training_twitter_x_train.csv\")\n",
    "testing = pd.read_csv(\"twitter_x_test.csv\")\n",
    "training.shape, testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg87TG-3oOe7"
   },
   "source": [
    "## **Cleaning up the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qO-A4Hd9ItS",
    "outputId": "b23afbef-ab08-454d-dd2e-0942e92f80c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 6851, 2327)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = training.copy()\n",
    "df_test = testing.copy()\n",
    "Negtive = len(df_train.airline_sentiment[df_train.airline_sentiment == 'negative'])\n",
    "Positive = len(df_train.airline_sentiment[df_train.airline_sentiment == 'positive'])\n",
    "Neutral = len(df_train.airline_sentiment[df_train.airline_sentiment == 'neutral'])\n",
    "Positive,Negtive,Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZLkkkipod-k"
   },
   "source": [
    "### **Exracting the tweets and sentiments of training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fumxf4EZ9MLo"
   },
   "outputs": [],
   "source": [
    "training_tweets=df_train['text'].values\n",
    "sentiments=df_train['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMd2kTEX-T_T",
    "outputId": "958eb838-7ab9-4c47-bf98-11e78da71171"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980,), (10980,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets.shape,sentiments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9pG4NiSotMs"
   },
   "source": [
    "### **Tokenizing the each tweet and extracting the sentiments**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "diy0A7tK-YmR"
   },
   "outputs": [],
   "source": [
    "train_docs=[]\n",
    "for i in range(len(training_tweets)):\n",
    "    train_docs.append((word_tokenize(training_tweets[i]),sentiments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yphU83Qi_izM",
    "outputId": "1505a98b-ce2a-4b87-c8ca-21b821ea711d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@',\n",
       "  'SouthwestAir',\n",
       "  'I',\n",
       "  'am',\n",
       "  'scheduled',\n",
       "  'for',\n",
       "  'the',\n",
       "  'morning',\n",
       "  ',',\n",
       "  '2',\n",
       "  'days',\n",
       "  'after',\n",
       "  'the',\n",
       "  'fact',\n",
       "  ',',\n",
       "  'yes',\n",
       "  '..',\n",
       "  'not',\n",
       "  'sure',\n",
       "  'why',\n",
       "  'my',\n",
       "  'evening',\n",
       "  'flight',\n",
       "  'was',\n",
       "  'the',\n",
       "  'only',\n",
       "  'one',\n",
       "  'Cancelled',\n",
       "  'Flightled'],\n",
       " 'negative')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is in tuple form with 1 list of tokenize words and other with sentiments\n",
    "train_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gjxrSO3j_mVQ"
   },
   "outputs": [],
   "source": [
    "# Funtion for converting pos_tag into the form feasible for lemmatizer\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXEKPh7kpWQC"
   },
   "source": [
    "### **Creating a list of stopwords and punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mLRHAoD3_u7p"
   },
   "outputs": [],
   "source": [
    "# Creating list which  \n",
    "stops=set(stopwords.words('english'))\n",
    "punctuations=list(string.punctuation)\n",
    "stops.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hNt4rAfKAiVU"
   },
   "outputs": [],
   "source": [
    "# object for lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3ZQY1NP2_wsY"
   },
   "outputs": [],
   "source": [
    "# function to clean each tweets\n",
    "def clean_tweet(words):\n",
    "\n",
    "    # Created empty a list to store the clean tweet words\n",
    "    output_words=[]\n",
    "\n",
    "    # traversing through each word \n",
    "    # Ignoring the stopwords and punctuations\n",
    "    # Checking whether the word contains only alphabets or not\n",
    "\n",
    "    for w in words:\n",
    "        if w.lower() not in stops and w.lower().isalpha():\n",
    "          \n",
    "            # calculating pos tag and applying lemmatizer \n",
    "            pos=pos_tag([w])\n",
    "            clean_word=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v7sFIFUnAKXD"
   },
   "outputs": [],
   "source": [
    "# Applying the clean_tweet function on training data\n",
    "train_docs = [(clean_tweet(tweet),sentiment) for tweet,sentiment in train_docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJUzvmQJASMD",
    "outputId": "851b3a02-e129-47bb-eb2d-a11c55162c3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['southwestair schedule morning day fact yes sure even flight one cancelled flightled',\n",
       " 'southwestair see worker time time go beyond love fly guy thank',\n",
       " 'united flew ord miami back great crew service leg thanks',\n",
       " 'southwestair horse radish',\n",
       " 'united flight ord delayed air force one last flight sbn min land',\n",
       " 'united load u fly sardine knew pilot hour late flight incompetent beyond belief',\n",
       " 'jetblue stock response delays frustrate poor cust serv amp told ppl wait amp come back',\n",
       " 'jetblue nice hoping rack enough mile take trip seattle enjoy perfect latte city coffee',\n",
       " 'united frankly bad customer service ever problems happen deal defines company never united',\n",
       " 'southwestair yeah haha never one expensive much fun destinationdragons',\n",
       " 'southwestair gt dca flight almost full people screw cancelled flightation united usairways cancelled flight',\n",
       " 'jetblue easy way get ticket receipt get one check get one online thanks',\n",
       " 'usairways love change lounge cheese veggie olive addition cracker snack mix',\n",
       " 'southwestair receive bad customer service end spending several hundred dollar accommodate family cxl flight',\n",
       " 'usairways kill sche flight board min mine arrive gate side airport genious',\n",
       " 'usairways week refund ticket totally unacceptable fix put blast social medium tv',\n",
       " 'americanair private msg provide detail u really need customer svc training staff',\n",
       " 'rt jetblue fleet fleek http',\n",
       " 'jetblue info flight sju jfk already delayed hour',\n",
       " 'americanair try less time today get touch service desk begin est luck']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the words of clean tweets\n",
    "training_tweet_docs =[\" \".join(tweet) for tweet,sentiment in train_docs]\n",
    "# First 20 cleaned tweets of training data\n",
    "training_tweet_docs[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7CEA5Syrmcz"
   },
   "source": [
    "### **Cleaning up the testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RPkTu_A2A-mK"
   },
   "outputs": [],
   "source": [
    "testing_tweets=df_test['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ-ytdQVr7-0"
   },
   "source": [
    "### **Tokenizing the each tweet**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVJ9U2CIB3le",
    "outputId": "b5d21a16-a2a2-4753-a44b-d017125b14fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'AmericanAir',\n",
       " 'In',\n",
       " 'car',\n",
       " 'gng',\n",
       " 'to',\n",
       " 'DFW',\n",
       " '.',\n",
       " 'Pulled',\n",
       " 'over',\n",
       " '1hr',\n",
       " 'ago',\n",
       " '-',\n",
       " 'very',\n",
       " 'icy',\n",
       " 'roads',\n",
       " '.',\n",
       " 'On-hold',\n",
       " 'with',\n",
       " 'AA',\n",
       " 'since',\n",
       " '1hr',\n",
       " '.',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " 'reach',\n",
       " 'arpt',\n",
       " 'for',\n",
       " 'AA2450',\n",
       " '.',\n",
       " 'Wat',\n",
       " '2',\n",
       " 'do',\n",
       " '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_docs=[]\n",
    "for i in range(len(testing_tweets)):\n",
    "    testing_docs.append((word_tokenize(testing_tweets[i])))\n",
    "testing_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCiJ5KIIsDdG"
   },
   "source": [
    "### **Cleaning up the tweets using clean_tweet()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0ofekBmCBh8",
    "outputId": "abafa4cb-5cbb-49a4-cf79-ebb8f753b5eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['americanair',\n",
       "  'car',\n",
       "  'gng',\n",
       "  'dfw',\n",
       "  'pulled',\n",
       "  'ago',\n",
       "  'icy',\n",
       "  'road',\n",
       "  'aa',\n",
       "  'since',\n",
       "  'ca',\n",
       "  'reach',\n",
       "  'arpt',\n",
       "  'wat'],\n",
       " 3660)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_docs =[clean_tweet(tweet) for tweet in testing_docs]\n",
    "testing_docs[0],len(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dh7l8e_7CLvp",
    "outputId": "228fd8c7-2fbf-4912-a6ba-6c2356f62f02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['americanair car gng dfw pulled ago icy road aa since ca reach arpt wat',\n",
       " 'americanair plane land identical bad condition grk accord metars',\n",
       " 'southwestair ca believe many pay customer left high dry reason flight cancelled flightlations monday bdl wow',\n",
       " 'usairways legitimately say would rather driven cross country flown us airways',\n",
       " 'americanair still response aa great job guy',\n",
       " 'united developer fly tmrw morn min layover earlier flight layover move',\n",
       " 'usairways hello anyone',\n",
       " 'usairways husainhaqqani husain u shld protest well one ur party member rehman malik delayed pia flight hour',\n",
       " 'usairways likely flightaware say plane still durango depart',\n",
       " 'americanair even give option hold say line busy plz try late flightr',\n",
       " 'united announcement pre boarding address mobility disability require travel lot stuff preboard',\n",
       " 'usairways really embarrass ask complimentary detailed http amp argue',\n",
       " 'southwestair passport time trip could still fly photo id thingsishouldknow ifeeldumb',\n",
       " 'americanair delayed bag friend lisa pafe get bag day costa rica issue update system',\n",
       " 'southwestair see travel compete unused fund expiration date hidden fine print never saw',\n",
       " 'usairways awesome doors close minute flight leaf minute plane get wth',\n",
       " 'united flew united last month experience awesome',\n",
       " 'jetblue accepting apple pay mobile enterprise http',\n",
       " 'united cab ride dfw love get bag reimburse',\n",
       " 'jetblue ill call morning upset right']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the words of clean tweets\n",
    "testing_tweet_docs=[\" \".join(tweet) for tweet in testing_docs]\n",
    "# First 20 cleaned tweets of testing data\n",
    "testing_tweet_docs[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jyhe_otcsqt8"
   },
   "source": [
    "## **Count Vectorizer, Tfid Vectorizer method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Jgu7XDDOIjkQ"
   },
   "outputs": [],
   "source": [
    "# tfid_vec = TfidfVectorizer(max_features=1100)\n",
    "# x_train = tfid_vec.fit_transform(training_tweet_docs)\n",
    "# y_train = sentiments\n",
    "# x_test = tfid_vec.transform(testing_tweet_docs)\n",
    "\n",
    "count_vec = TfidfVectorizer(max_features=1100)\n",
    "x_train = count_vec.fit_transform(training_tweet_docs)\n",
    "y_train = sentiments\n",
    "x_test = count_vec.transform(testing_tweet_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5l464JTbJ4cr",
    "outputId": "1f3d860c-ff10-49a5-a325-3a114493bdd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1500, n_jobs=-1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Random Classifier and fitting with training data\n",
    "rf = RandomForestClassifier(n_estimators=1500, n_jobs=-1)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908014571948999"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7585610200364299"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(rf,x_train , y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ql5KwVv9J6CI"
   },
   "outputs": [],
   "source": [
    "# predicting on testing data and here y_pred = y_test (we want to predict directly the target so we cant calculate the score on it) \n",
    "y_test = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "JqexP3TaKfDU"
   },
   "outputs": [],
   "source": [
    "# Converting predictions into the csv file\n",
    "np.savetxt(\"twitter_predictions.csv\", y_test, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, kernel='linear')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel = 'linear',C=1000)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773224043715847"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf,x_train , y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [1, 10, 100, 500, 1000, 5000],\n",
       "                         'gamma': [0.001, 0.0005, 0.0001, 0.005]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {'C':[1,10,100,500,1000,5000],'gamma':[1e-3,5e-4,1e-4,5e-3]}\n",
    "abc = GridSearchCV(clf,grid)\n",
    "abc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5000, gamma=0.0001)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"twitter_predictions.csv\", y_test, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf3 = MultinomialNB()\n",
    "clf3.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768943533697632"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7436247723132968"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf3,x_train,y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"twitter_predictions.csv\", y_test, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
